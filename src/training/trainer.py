"""
Training utilities for MTSC models
"""

import time

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score, f1_score
from torch.utils.data import DataLoader, TensorDataset


class Trainer:
    """Model trainer with comprehensive error handling"""

    def __init__(self, device="auto"):
        if device == "auto":
            if torch.cuda.is_available():
                self.device = "cuda"
            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                self.device = "mps"
            else:
                self.device = "cpu"
        else:
            self.device = device

        print(f"ðŸ”§ Trainer using device: {self.device}")

    def train_and_evaluate(
        self, model, X_train, y_train, X_test, y_test, epochs=15, lr=1e-3, batch_size=32
    ):
        """Train model and return evaluation metrics"""

        # Move model to device
        model = model.to(self.device)

        # Adjust batch size for small datasets
        batch_size = min(batch_size, len(X_train) // 4, 32)
        if batch_size < 2:
            batch_size = 2

        # Create data loaders
        train_dataset = TensorDataset(
            torch.FloatTensor(X_train), torch.LongTensor(y_train)
        )
        test_dataset = TensorDataset(
            torch.FloatTensor(X_test), torch.LongTensor(y_test)
        )

        train_loader = DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True, drop_last=True
        )
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

        # Setup training components
        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
        criterion = nn.CrossEntropyLoss()
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode="min", patience=3, factor=0.5
        )

        # Training loop
        model.train()
        for epoch in range(epochs):
            epoch_loss = 0
            num_batches = 0

            for batch_x, batch_y in train_loader:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)

                optimizer.zero_grad()

                try:
                    # Forward pass
                    outputs = model(batch_x)
                    loss = criterion(outputs, batch_y)

                    # Check for invalid loss
                    if torch.isnan(loss) or torch.isinf(loss):
                        continue

                    # Backward pass
                    loss.backward()

                    # Gradient clipping
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                    optimizer.step()

                    epoch_loss += loss.item()
                    num_batches += 1

                except Exception as e:
                    # Skip problematic batches
                    continue

            # Update learning rate
            if num_batches > 0:
                avg_loss = epoch_loss / num_batches
                scheduler.step(avg_loss)

        # Evaluation
        return self._evaluate(model, test_loader, y_test)

    def _evaluate(self, model, test_loader, y_test):
        """Evaluate model performance"""
        model.eval()
        all_preds = []
        all_labels = []

        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x = batch_x.to(self.device)

                try:
                    outputs = model(batch_x)
                    preds = outputs.argmax(dim=1).cpu().numpy()
                    all_preds.extend(preds)
                    all_labels.extend(batch_y.numpy())
                except Exception as e:
                    # Use random predictions as fallback
                    n_classes = len(torch.unique(torch.LongTensor(y_test)))
                    random_preds = torch.randint(0, n_classes, (len(batch_y),))
                    all_preds.extend(random_preds.numpy())
                    all_labels.extend(batch_y.numpy())

        # Calculate metrics
        if len(all_preds) == 0:
            return 0.0, 0.0

        accuracy = accuracy_score(all_labels, all_preds)
        f1 = f1_score(all_labels, all_preds, average="weighted", zero_division=0)

        return accuracy, f1
